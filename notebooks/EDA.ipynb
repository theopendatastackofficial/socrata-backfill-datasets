{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import duckdb\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "\n",
    "try:\n",
    "    from IPython.display import display, HTML\n",
    "    IN_NOTEBOOK = True\n",
    "except ImportError:\n",
    "    IN_NOTEBOOK = False\n",
    "\n",
    "try:\n",
    "    from great_tables import loc, style\n",
    "    HAS_GREAT_TABLES = True\n",
    "except ImportError:\n",
    "    HAS_GREAT_TABLES = False\n",
    "\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "class DuckDBWrapper:\n",
    "    def __init__(self, duckdb_path=None):\n",
    "        \"\"\"\n",
    "        Initialize a DuckDB connection.\n",
    "        If duckdb_path is provided, a persistent DuckDB database will be used.\n",
    "        Otherwise, it creates an in-memory database.\n",
    "        \"\"\"\n",
    "        if duckdb_path:\n",
    "            self.con = duckdb.connect(str(duckdb_path), read_only=False)\n",
    "        else:\n",
    "            self.con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "        self.registered_tables = []\n",
    "\n",
    "        # Enable httpfs for remote paths if needed\n",
    "        self.con.execute(\"INSTALL httpfs;\")\n",
    "        self.con.execute(\"LOAD httpfs;\")\n",
    "\n",
    "    def register_data(self, paths, table_names, show_tables=False):\n",
    "        \"\"\"\n",
    "        Registers local data files (Parquet, CSV, JSON) in DuckDB by creating views.\n",
    "        Automatically detects file type (parquet, csv, json).\n",
    "        \n",
    "        If any table fails to register (I/O error, missing file, etc.), \n",
    "        it logs the error and continues with the others.\n",
    "\n",
    "        Args:\n",
    "            paths (list[str | Path]): List of file paths to register.\n",
    "            table_names (list[str]): Corresponding table names.\n",
    "            show_tables (bool): If True, display table catalog after registration.\n",
    "        \"\"\"\n",
    "        if len(paths) != len(table_names):\n",
    "            raise ValueError(\"The number of paths must match the number of table names.\")\n",
    "\n",
    "        for path, table_name in zip(paths, table_names):\n",
    "            path_str = str(path)\n",
    "            file_extension = Path(path_str).suffix.lower()\n",
    "\n",
    "            try:\n",
    "                if file_extension == \".parquet\":\n",
    "                    query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_parquet('{path_str}')\"\n",
    "                elif file_extension == \".csv\":\n",
    "                    query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_csv_auto('{path_str}')\"\n",
    "                elif file_extension == \".json\":\n",
    "                    query = f\"CREATE VIEW {table_name} AS SELECT * FROM read_json_auto('{path_str}')\"\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported file type '{file_extension}' for file: {path_str}\")\n",
    "\n",
    "                self.con.execute(query)\n",
    "                self.registered_tables.append(table_name)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to register '{table_name}' from '{path_str}': {e}. Skipping...\")\n",
    "\n",
    "        # Optionally show tables\n",
    "        if show_tables:\n",
    "            self.show_tables()\n",
    "\n",
    "    def bulk_register_data(self, repo_root, base_path, table_names, wildcard=\"*.parquet\", show_tables=False):\n",
    "        \"\"\"\n",
    "        Constructs paths for each table based on a shared base path plus the table name,\n",
    "        then appends a wildcard for file matching (e.g., '*.parquet'), and registers the data.\n",
    "        \n",
    "        Args:\n",
    "            repo_root (Path | str): The root path of your repository.\n",
    "            base_path (str): The relative path from repo_root to your data directory.\n",
    "            table_names (list[str]): The table names (and folder names) to register.\n",
    "            wildcard (str): A wildcard pattern for the files (default '*.parquet').\n",
    "            show_tables (bool): If True, display table catalog after registration.\n",
    "        \"\"\"\n",
    "        paths = []\n",
    "        for table_name in table_names:\n",
    "            path = Path(repo_root) / base_path / table_name / wildcard\n",
    "            paths.append(path)\n",
    "\n",
    "        self.register_data(paths, table_names, show_tables=False)\n",
    "\n",
    "        if show_tables:\n",
    "            self.show_tables()\n",
    "\n",
    "    def register_partitioned_data(self, base_path, table_name, wildcard=\"*/*/*.parquet\", show_tables=False):\n",
    "        \"\"\"\n",
    "        Registers partitioned Parquet data using Hive partitioning by creating a view.\n",
    "\n",
    "        Args:\n",
    "            base_path (str | Path): The base directory where partitioned files are located.\n",
    "            table_name (str): Name of the view to be created.\n",
    "            wildcard (str): Glob pattern to locate the parquet files (default '*/*/*.parquet').\n",
    "            show_tables (bool): If True, display table catalog after registration.\n",
    "        \"\"\"\n",
    "        path_str = str(Path(base_path) / wildcard)\n",
    "\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            CREATE OR REPLACE VIEW {table_name} AS \n",
    "            SELECT * FROM read_parquet('{path_str}', hive_partitioning=true)\n",
    "            \"\"\"\n",
    "            self.con.execute(query)\n",
    "            self.registered_tables.append(table_name)\n",
    "            print(f\"Partitioned view '{table_name}' created for files at '{path_str}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to register partitioned data for '{table_name}': {e}. Skipping...\")\n",
    "\n",
    "        if show_tables:\n",
    "            self.show_tables()\n",
    "\n",
    "    def bulk_register_partitioned_data(self, repo_root, base_path, table_names, wildcard=\"*/*/*.parquet\", show_tables=False):\n",
    "        \"\"\"\n",
    "        Registers multiple partitioned Parquet datasets, using Hive partitioning, by looping \n",
    "        over a list of table names. For each name, it constructs a path from (repo_root / base_path / table_name),\n",
    "        then appends the wildcard, and calls register_partitioned_data.\n",
    "\n",
    "        Args:\n",
    "            repo_root (Path | str): The root path of your repository.\n",
    "            base_path (str): The relative path from repo_root to your partitioned datasets.\n",
    "            table_names (list[str]): List of partitioned datasets (folder names) to register.\n",
    "            wildcard (str): The glob pattern to locate the .parquet files. Default '*/*/*.parquet'.\n",
    "            show_tables (bool): If True, displays the table catalog after registration.\n",
    "        \"\"\"\n",
    "        for table_name in table_names:\n",
    "            partition_path = Path(repo_root) / base_path / table_name\n",
    "            self.register_partitioned_data(\n",
    "                base_path=partition_path,\n",
    "                table_name=table_name,\n",
    "                wildcard=wildcard,\n",
    "                show_tables=False  # We'll show_tables once at the end\n",
    "            )\n",
    "\n",
    "        if show_tables:\n",
    "            self.show_tables()\n",
    "\n",
    "    def run_query(self, sql_query, show_results=False):\n",
    "        \"\"\"\n",
    "        Runs a SQL query on the registered tables in DuckDB and returns a Polars DataFrame.\n",
    "        Optionally displays the result. \n",
    "        \"\"\"\n",
    "        arrow_table = self.con.execute(sql_query).arrow()\n",
    "        df = pl.DataFrame(arrow_table)\n",
    "\n",
    "        if show_results:\n",
    "            if IN_NOTEBOOK and HAS_GREAT_TABLES:\n",
    "                styled = (\n",
    "                    df.style\n",
    "                    .tab_header(\n",
    "                        title=\"DuckDB Query Results\",\n",
    "                        subtitle=f\"{sql_query[:50]}...\"\n",
    "                    )\n",
    "                    .fmt_number(cs.numeric(), decimals=3)\n",
    "                )\n",
    "                styled_html = styled._repr_html_()\n",
    "                scrollable_html = f\"\"\"\n",
    "                <div style=\"max-width:100%; overflow-x:auto; white-space:nowrap;\">\n",
    "                    {styled_html}\n",
    "                </div>\n",
    "                \"\"\"\n",
    "                display(HTML(scrollable_html))\n",
    "            else:\n",
    "                self.print_query_results(df, title=f\"Query: {sql_query[:50]}...\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def print_query_results(self, df, title=\"Query Results\"):\n",
    "        \"\"\"\n",
    "        Prints a Polars DataFrame as a Rich table.\n",
    "        Uses a pager for large outputs in the terminal.\n",
    "        \"\"\"\n",
    "        console = Console()\n",
    "        with console.pager(styles=True):\n",
    "            table = Table(title=title, title_style=\"bold green\", show_lines=True)\n",
    "            for column in df.columns:\n",
    "                table.add_column(str(column), style=\"bold cyan\", overflow=\"fold\")\n",
    "\n",
    "            for row in df.iter_rows(named=True):\n",
    "                values = [str(row[col]) for col in df.columns]\n",
    "                table.add_row(*values, style=\"white on black\")\n",
    "\n",
    "            console.print(table)\n",
    "\n",
    "    def _construct_path(self, path, base_path, file_name, extension):\n",
    "        \"\"\"\n",
    "        Constructs the full file path based on input parameters.\n",
    "        \"\"\"\n",
    "        if path:\n",
    "            return Path(path)\n",
    "        elif base_path and file_name:\n",
    "            return Path(base_path) / f\"{file_name}.{extension}\"\n",
    "        else:\n",
    "            return Path(f\"output.{extension}\")\n",
    "\n",
    "    def export(self, result, file_type, path=None, base_path=None, file_name=None, with_header=True):\n",
    "        \"\"\"\n",
    "        Exports a Polars DataFrame (or anything convertible to Polars) \n",
    "        to the specified file type (parquet, csv, json).\n",
    "        \"\"\"\n",
    "        file_type = file_type.lower()\n",
    "        if file_type not in [\"parquet\", \"csv\", \"json\"]:\n",
    "            raise ValueError(\"file_type must be one of 'parquet', 'csv', or 'json'.\")\n",
    "\n",
    "        full_path = self._construct_path(path, base_path, file_name, file_type)\n",
    "        full_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if isinstance(result, pl.DataFrame):\n",
    "            df = result\n",
    "        elif hasattr(result, \"to_arrow\"):\n",
    "            df = pl.DataFrame(result.to_arrow())\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported result type. Must be a Polars DataFrame or have a 'to_arrow()' method.\")\n",
    "\n",
    "        if file_type == \"parquet\":\n",
    "            df.write_parquet(str(full_path))\n",
    "        elif file_type == \"csv\":\n",
    "            df.write_csv(str(full_path), separator=\",\", include_header=with_header)\n",
    "        elif file_type == \"json\":\n",
    "            df.write_ndjson(str(full_path))\n",
    "\n",
    "        print(f\"File written to: {full_path}\")\n",
    "\n",
    "    def show_tables(self):\n",
    "        \"\"\"\n",
    "        Displays the table names and types currently registered in the catalog,\n",
    "        in a Rich-styled table.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT table_name, table_type\n",
    "        FROM information_schema.tables\n",
    "        WHERE table_schema='main'\n",
    "        \"\"\"\n",
    "        df = self.run_query(query)\n",
    "        console = Console()\n",
    "        table = Table(title=\"Registered Tables\", title_style=\"bold green\", show_lines=True)\n",
    "        table.add_column(\"Table Name\", justify=\"left\", style=\"bold yellow\")\n",
    "        table.add_column(\"Table Type\", justify=\"left\", style=\"bold cyan\")\n",
    "\n",
    "        for row in df.to_dicts():\n",
    "            table.add_row(row[\"table_name\"], row[\"table_type\"], style=\"white on black\")\n",
    "\n",
    "        console.print(table)\n",
    "\n",
    "    def show_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Displays the schema of the specified DuckDB table or view, using Rich formatting.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT column_name, data_type\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_name = '{table_name}'\n",
    "        \"\"\"\n",
    "        df = self.run_query(query)\n",
    "        console = Console()\n",
    "        schema_table = Table(title=f\"Schema for '{table_name}'\", title_style=\"bold green\")\n",
    "        schema_table.add_column(\"Column Name\", justify=\"left\", style=\"bold yellow\", no_wrap=True)\n",
    "        schema_table.add_column(\"Data Type\", justify=\"left\", style=\"bold cyan\")\n",
    "\n",
    "        for row in df.to_dicts():\n",
    "            schema_table.add_row(row[\"column_name\"], str(row[\"data_type\"]), style=\"white on black\")\n",
    "\n",
    "        console.print(schema_table)\n",
    "\n",
    "    def show_parquet_schema(self, file_path):\n",
    "        \"\"\"\n",
    "        Reads a Parquet file directly using Polars, and prints its schema \n",
    "        and row count using Rich formatting.\n",
    "        \"\"\"\n",
    "        df = pl.read_parquet(file_path)\n",
    "\n",
    "        console = Console()\n",
    "        schema_table = Table(title=\"Parquet Schema\", title_style=\"bold green\")\n",
    "        schema_table.add_column(\"Column Name\", justify=\"left\", style=\"bold yellow\", no_wrap=True)\n",
    "        schema_table.add_column(\"Data Type\", justify=\"left\", style=\"bold cyan\")\n",
    "\n",
    "        for col_name, col_dtype in df.schema.items():\n",
    "            schema_table.add_row(col_name, str(col_dtype), style=\"white on black\")\n",
    "\n",
    "        console.print(schema_table)\n",
    "        console.print(f\"[bold magenta]\\nNumber of rows:[/] [bold white]{df.height}[/]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DuckDBWrapper (in-memory DuckDB instance) You can connect directly to a DuckDB file by adding the path like con = DuckDBWrapper()\n",
    "con = DuckDBWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "partitioned_table_names = [\n",
    "    \"nyc_threeoneone_requests\",\n",
    "    \"mta_subway_origin_destination_2023\",\n",
    "    \"mta_subway_origin_destination_2024\",\n",
    "    \"mta_subway_hourly_ridership\"\n",
    "]\n",
    "\n",
    "# Suppose your directory structure is something like:\n",
    "repo_root = Path.cwd().resolve().parents[0]  # Adjust to locate the repo root\n",
    "base_path = \"data/opendata\"\n",
    "\n",
    "con.bulk_register_partitioned_data(\n",
    "    repo_root=repo_root,\n",
    "    base_path=base_path,\n",
    "    table_names=partitioned_table_names,\n",
    "    wildcard=\"year=*/month=*/*.parquet\",\n",
    "    show_tables=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the Parquet file\n",
    "file_path = \"/home/christiandata/transittechiesdemo/data/opendata/mta_subway_origin_destination_2024\"\n",
    "# Show schema\n",
    "con.show_parquet_schema(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "\n",
    "SELECT \n",
    "count(*) as rows, \n",
    "MIN(created_date) as min,\n",
    "MAX(created_date) as max\n",
    "from \n",
    "nyc_threeoneone_requests\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want a better looking table, set show_results=True. I'd recomend capping the limit at about 50 rows\n",
    "#T\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    " * from \n",
    "crime_nypd_arrests limit 10\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query,show_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More complicated query\n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "WITH weekly_ridership AS (\n",
    "    SELECT \n",
    "        station_complex, \n",
    "        DATE_TRUNC('week', transit_timestamp) AS week_start,\n",
    "        SUM(ridership) AS total_weekly_ridership,\n",
    "        MIN(latitude) AS latitude,  -- Assuming latitude is the same for each station complex, use MIN() or MAX()\n",
    "        MIN(longitude) AS longitude  -- Assuming longitude is the same for each station complex, use MIN() or MAX()\n",
    "    FROM \n",
    "        mta_hourly_subway_socrata\n",
    "    GROUP BY \n",
    "        station_complex, \n",
    "        DATE_TRUNC('week', transit_timestamp)\n",
    "),\n",
    "weekly_weather AS (\n",
    "    SELECT \n",
    "        DATE_TRUNC('week', date) AS week_start,\n",
    "        AVG(temperature_mean) AS avg_weekly_temperature,\n",
    "        SUM(precipitation_sum) AS total_weekly_precipitation\n",
    "    FROM \n",
    "        daily_weather_asset\n",
    "    GROUP BY \n",
    "        DATE_TRUNC('week', date)\n",
    ")\n",
    "SELECT \n",
    "    wr.station_complex, \n",
    "    wr.week_start, \n",
    "    wr.total_weekly_ridership,\n",
    "    wr.latitude,\n",
    "    wr.longitude,\n",
    "    ww.avg_weekly_temperature,\n",
    "    ww.total_weekly_precipitation\n",
    "FROM \n",
    "    weekly_ridership wr\n",
    "LEFT JOIN \n",
    "    weekly_weather ww\n",
    "ON \n",
    "    wr.week_start = ww.week_start\n",
    "WHERE \n",
    "    wr.week_start < '2024-09-17'\n",
    "ORDER BY \n",
    "    wr.station_complex, \n",
    "    wr.week_start\n",
    " \n",
    "\n",
    "LIMIT 50\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query,show_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the tables registered\n",
    "con.show_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the schema of a specific table\n",
    "con.show_schema(\"mta_hourly_subway_socrata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = Path.cwd().resolve().parents[0]  # Adjust to locate the repo root\n",
    "base_path = repo_root / \"data/exports\"\n",
    "file_name = \"mta_hourly_subway_socrata_data_sample\"\n",
    "file_type= \"csv\"\n",
    "# Export the query result to CSV\n",
    "con.export(result, file_type=file_type, base_path=base_path, file_name=file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
